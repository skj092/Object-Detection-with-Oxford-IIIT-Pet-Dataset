{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwscfBOejSES6bieqJpveb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skj092/Object-Detection-with-Oxford-IIIT-Pet-Dataset/blob/main/iiit_object_detection_pets_fastai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WtDCX3OXhh2S"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.PETS)\n",
        "Path.BASE_PATH  = path\n",
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PgzndBWhtP6",
        "outputId": "178a53cb-77a5-40a5-ec47-020b48682107"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('annotations'),Path('images')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "class CustomObjectDetectionDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.xml_files = [file for file in os.listdir(os.path.join(path, 'annotations/xmls')) if file.endswith('.xml')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xml_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        xml_file = os.path.join(self.root_dir, 'annotations/xmls', self.xml_files[idx])\n",
        "        img_name = os.path.splitext(self.xml_files[idx])[0] + '.jpg'\n",
        "        img_path = os.path.join(self.root_dir, 'images', img_name)\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path).convert(\"RGB\").resize((224,224))\n",
        "\n",
        "        # Load and parse XML annotation\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract image size\n",
        "        width = int(root.find('size').find('width').text)\n",
        "        height = int(root.find('size').find('height').text)\n",
        "\n",
        "        # Initialize lists for target data\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        # Extract bounding box information\n",
        "        for obj in root.findall('object'):\n",
        "            label = obj.find('name').text\n",
        "            xmin = int(obj.find('bndbox').find('xmin').text)\n",
        "            ymin = int(obj.find('bndbox').find('ymin').text)\n",
        "            xmax = int(obj.find('bndbox').find('xmax').text)\n",
        "            ymax = int(obj.find('bndbox').find('ymax').text)\n",
        "\n",
        "            # Append bounding box coordinates and label\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(label)\n",
        "\n",
        "        # Convert boxes and labels to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor([labels.index(label) for label in labels], dtype=torch.int64)\n",
        "\n",
        "        # Calculate area (optional)\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # Define iscrowd (optional)\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "        # Create target dictionary\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            img, target = self.transform(img, target)\n",
        "\n",
        "        return img, target\n"
      ],
      "metadata": {
        "id": "KqUW4m0fqlIG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_dataset_sample(dataset, index, class_names=None):\n",
        "    \"\"\"\n",
        "    Visualizes a single data sample from a dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The PyTorch dataset containing the data samples.\n",
        "        index (int): The index of the data sample to visualize.\n",
        "        class_names (list): Optional list of class names corresponding to label indices.\n",
        "    \"\"\"\n",
        "    sample = dataset[index]\n",
        "    image, target = sample\n",
        "\n",
        "    image = np.array(image)\n",
        "    boxes = target[\"boxes\"].numpy()\n",
        "    labels = target[\"labels\"].numpy()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    for box, label in zip(boxes, labels):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        rect = plt.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin, fill=False, edgecolor='red', linewidth=2)\n",
        "        plt.gca().add_patch(rect)\n",
        "\n",
        "        if class_names:\n",
        "            label_name = class_names[label]\n",
        "        else:\n",
        "            label_name = str(label)\n",
        "\n",
        "        plt.text(xmin, ymin, label_name, backgroundcolor='red', color='white', fontsize=8)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aufJe_dRtz9w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize_dataset_sample(ds, 1, class_names=['dog', 'cat'])"
      ],
      "metadata": {
        "id": "tT0I2Keet_oF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float))\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "mlS1N29EuL0r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8n5hKJGxgOH",
        "outputId": "98db452d-0edb-4a3c-afa5-bef8c820c71d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader, Subset\n",
        "\n",
        "ds = CustomObjectDetectionDataset(path, transform=get_transform(train=True))\n",
        "\n",
        "\n",
        "# Define the dataset size and the desired split ratio\n",
        "dataset_size = len(ds)\n",
        "validation_split = 0.2  # 20% of the data will be used for validation\n",
        "\n",
        "# Calculate the sizes of the training and validation sets\n",
        "valid_size = int(validation_split * dataset_size)\n",
        "train_size = dataset_size - valid_size\n",
        "\n",
        "# Use random_split to split the dataset into train and validation subsets\n",
        "train_subset, valid_subset = random_split(ds, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoader objects for train and validation sets\n",
        "batch_size = 32  # You can adjust this to your preference\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optionally, you can create a complete dataset for train and validation\n",
        "train_ds = Subset(ds, train_subset.indices)\n",
        "valid_ds = Subset(ds, valid_subset.indices)\n",
        "\n",
        "len(train_ds), len(valid_ds)"
      ],
      "metadata": {
        "id": "qJhlJlHsYP8M",
        "outputId": "8b1d1aec-8272-40b3-bd3b-e3a2aa41d777",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2949, 737)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_dl=DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=2,\n",
        "                    pin_memory=True if torch.cuda.is_available else False,\n",
        "                    collate_fn=collate_fn)\n",
        "val_dl=DataLoader(valid_ds,batch_size=batch_size,shuffle=False,num_workers=2,\n",
        "                  pin_memory=True if torch.cuda.is_available else False,\n",
        "                  collate_fn=collate_fn)\n",
        "\n",
        "len(train_dl), len(val_dl)\n"
      ],
      "metadata": {
        "id": "bFduVeBBXzPX",
        "outputId": "6d2ea267-8094-43b5-9dbd-540d945cbcf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(93, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = next(iter(train_dl))\n",
        "xb[0], yb[0]"
      ],
      "metadata": {
        "id": "qoYuBMwLYdnB",
        "outputId": "b7b29a1c-bf69-4388-dec2-c8ef0f2c2649",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.5098, 0.4078, 0.4039,  ..., 0.3725, 0.3216, 0.1961],\n",
              "          [0.5059, 0.4471, 0.3961,  ..., 0.3490, 0.3176, 0.1725],\n",
              "          [0.4980, 0.4706, 0.3804,  ..., 0.3490, 0.3059, 0.1725],\n",
              "          ...,\n",
              "          [0.0627, 0.0549, 0.0471,  ..., 0.1020, 0.1020, 0.1294],\n",
              "          [0.0627, 0.0353, 0.0549,  ..., 0.1020, 0.0980, 0.1176],\n",
              "          [0.0627, 0.0392, 0.0549,  ..., 0.1137, 0.1137, 0.1216]],\n",
              " \n",
              "         [[0.6353, 0.4627, 0.3294,  ..., 0.3098, 0.2392, 0.1137],\n",
              "          [0.6314, 0.5098, 0.3333,  ..., 0.2863, 0.2353, 0.0980],\n",
              "          [0.6196, 0.5529, 0.3373,  ..., 0.2824, 0.2196, 0.1059],\n",
              "          ...,\n",
              "          [0.0980, 0.1020, 0.1059,  ..., 0.1294, 0.1333, 0.1373],\n",
              "          [0.1059, 0.0980, 0.1098,  ..., 0.1373, 0.1333, 0.1255],\n",
              "          [0.0980, 0.1059, 0.1059,  ..., 0.1490, 0.1412, 0.1333]],\n",
              " \n",
              "         [[0.6745, 0.4196, 0.1922,  ..., 0.2196, 0.1882, 0.1020],\n",
              "          [0.6706, 0.4745, 0.2078,  ..., 0.1961, 0.1804, 0.0745],\n",
              "          [0.6471, 0.5294, 0.2235,  ..., 0.1961, 0.1608, 0.0706],\n",
              "          ...,\n",
              "          [0.1412, 0.1451, 0.1333,  ..., 0.1843, 0.1882, 0.2039],\n",
              "          [0.1412, 0.1294, 0.1451,  ..., 0.1804, 0.1882, 0.2000],\n",
              "          [0.1373, 0.1333, 0.1451,  ..., 0.1882, 0.2039, 0.2118]]]),\n",
              " {'boxes': tensor([[145.,   7., 284., 143.]]),\n",
              "  'labels': tensor([0]),\n",
              "  'image_id': tensor([401]),\n",
              "  'area': tensor([18904.]),\n",
              "  'iscrowd': tensor([0])})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 3  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "65sPnhYd6BVp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
        "# ``FasterRCNN`` needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "    featmap_names=['0'],\n",
        "    output_size=7,\n",
        "    sampling_ratio=2,\n",
        ")\n",
        "\n",
        "# put the pieces together inside a Faster-RCNN model\n",
        "model = FasterRCNN(\n",
        "    backbone,\n",
        "    num_classes=3,\n",
        "    rpn_anchor_generator=anchor_generator,\n",
        "    box_roi_pool=roi_pooler,\n",
        ")"
      ],
      "metadata": {
        "id": "Gy-UIfQraEm4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import utils\n",
        "\n",
        "\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# # For Training\n",
        "# images, targets = next(iter(train_dl))\n",
        "# images = list(image for image in images)\n",
        "# targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "# output = model(images, targets)  # Returns losses and detections\n",
        "# print(output)\n",
        "\n",
        "# # For inference\n",
        "# model.eval()\n",
        "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "# predictions = model(x)  # Returns predictions\n",
        "# print(predictions[0])"
      ],
      "metadata": {
        "id": "h0MNzLOraTXN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "from torch.utils.data import Subset, random_split\n",
        "\n",
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 3\n",
        "# use our dataset and defined transformations\n",
        "ds = CustomObjectDetectionDataset(path, transform=get_transform(train=True))\n",
        "ds = Subset(ds, range(500))\n",
        "\n",
        "\n",
        "# Define the dataset size and the desired split ratio\n",
        "dataset_size = len(ds)\n",
        "validation_split = 0.2  # 20% of the data will be used for validation\n",
        "\n",
        "# Calculate the sizes of the training and validation sets\n",
        "valid_size = int(validation_split * dataset_size)\n",
        "train_size = dataset_size - valid_size\n",
        "\n",
        "# Use random_split to split the dataset into train and validation subsets\n",
        "train_subset, valid_subset = random_split(ds, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoader objects for train and validation sets\n",
        "batch_size = 4  # You can adjust this to your preference\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optionally, you can create a complete dataset for train and validation\n",
        "train_ds = Subset(ds, train_subset.indices)\n",
        "valid_ds = Subset(ds, valid_subset.indices)\n",
        "\n",
        "train_dl=DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=2,\n",
        "                    pin_memory=True if torch.cuda.is_available else False,\n",
        "                    collate_fn=collate_fn)\n",
        "val_dl=DataLoader(valid_ds,batch_size=batch_size,shuffle=False,num_workers=2,\n",
        "                  pin_memory=True if torch.cuda.is_available else False,\n",
        "                  collate_fn=collate_fn)\n",
        "\n",
        "# get the model using our helper function\n",
        "# model = get_model_instance_segmentation(num_classes)\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# let's train it for 5 epochs\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    # evaluate(model, val_dl, device=device)\n",
        "\n",
        "print(\"That's it!\")"
      ],
      "metadata": {
        "id": "lexY_vVJac4g",
        "outputId": "760bcd40-a114-4c55-cef1-f9e8cc4bef18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [  0/100]  eta: 0:01:56  lr: 0.000055  loss: 1.8654 (1.8654)  loss_classifier: 1.0862 (1.0862)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.7220 (0.7220)  loss_rpn_box_reg: 0.0572 (0.0572)  time: 1.1624  data: 0.1864  max mem: 5586\n",
            "Epoch: [0]  [ 10/100]  eta: 0:00:57  lr: 0.000560  loss: 1.8076 (3.4857)  loss_classifier: 1.0524 (1.0219)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.7114 (0.7116)  loss_rpn_box_reg: 0.0327 (1.7522)  time: 0.6422  data: 0.0197  max mem: 5903\n",
            "Epoch: [0]  [ 20/100]  eta: 0:00:49  lr: 0.001065  loss: 1.5803 (2.4634)  loss_classifier: 0.8116 (0.7958)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.6951 (0.6862)  loss_rpn_box_reg: 0.0440 (0.9813)  time: 0.5960  data: 0.0024  max mem: 5903\n",
            "Epoch: [0]  [ 30/100]  eta: 0:00:43  lr: 0.001569  loss: 0.9486 (1.9282)  loss_classifier: 0.2615 (0.5992)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.6105 (0.6424)  loss_rpn_box_reg: 0.0680 (0.6866)  time: 0.6013  data: 0.0014  max mem: 5903\n",
            "Epoch: [0]  [ 40/100]  eta: 0:00:36  lr: 0.002074  loss: 0.6955 (2.1008)  loss_classifier: 0.1095 (0.4674)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.4982 (0.5962)  loss_rpn_box_reg: 0.0674 (1.0371)  time: 0.6078  data: 0.0016  max mem: 5903\n",
            "Epoch: [0]  [ 50/100]  eta: 0:00:30  lr: 0.002578  loss: 0.4759 (1.7733)  loss_classifier: 0.0233 (0.3788)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.3828 (0.5434)  loss_rpn_box_reg: 0.0674 (0.8511)  time: 0.6116  data: 0.0021  max mem: 5903\n",
            "Epoch: [0]  [ 60/100]  eta: 0:00:24  lr: 0.003083  loss: 0.3729 (1.5436)  loss_classifier: 0.0090 (0.3177)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.2896 (0.4964)  loss_rpn_box_reg: 0.0601 (0.7295)  time: 0.6068  data: 0.0019  max mem: 5903\n",
            "Epoch: [0]  [ 70/100]  eta: 0:00:18  lr: 0.003587  loss: 0.2855 (1.4001)  loss_classifier: 0.0040 (0.2732)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.2233 (0.4594)  loss_rpn_box_reg: 0.0582 (0.6675)  time: 0.6011  data: 0.0017  max mem: 5903\n",
            "Epoch: [0]  [ 80/100]  eta: 0:00:12  lr: 0.004092  loss: 0.2425 (1.2958)  loss_classifier: 0.0020 (0.2398)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.1904 (0.4300)  loss_rpn_box_reg: 0.0497 (0.6261)  time: 0.5934  data: 0.0018  max mem: 5903\n",
            "Epoch: [0]  [ 90/100]  eta: 0:00:06  lr: 0.004596  loss: 0.2296 (1.2135)  loss_classifier: 0.0017 (0.2137)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.1755 (0.4056)  loss_rpn_box_reg: 0.0623 (0.5942)  time: 0.5912  data: 0.0021  max mem: 5903\n",
            "Epoch: [0]  [ 99/100]  eta: 0:00:00  lr: 0.005000  loss: 0.2296 (1.1273)  loss_classifier: 0.0015 (0.1945)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.1579 (0.3839)  loss_rpn_box_reg: 0.0777 (0.5489)  time: 0.5885  data: 0.0018  max mem: 5903\n",
            "Epoch: [0] Total time: 0:01:00 (0.6056 s / it)\n",
            "That's it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNHOHpkhddFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}